---
title: SAM (Segment Anything Model) Tool
description: Browser-based interactive segmentation using Meta's SAM via ONNX Runtime Web
---

# SAM (Segment Anything Model) Tool

The SAM tool brings Meta AI's powerful Segment Anything Model directly into your browser, enabling instant object segmentation without requiring any backend infrastructure.

## Overview

**Key Features:**
- ✅ **Zero latency** - Runs entirely in browser using WebAssembly
- ✅ **No backend required** - Pure client-side inference
- ✅ **Privacy-first** - Images never leave the device
- ✅ **Offline support** - Works without internet after model download
- ✅ **Cost-effective** - No GPU server costs
- ✅ **Real-time preview** - Hover to see segmentation before clicking
- ✅ **Smart caching** - Reuses preview mask when clicking to avoid redundant inference

**How it works:**
1. User hovers over an object - blue preview overlay appears
2. User clicks on the preview to create annotation
3. Cached mask is converted to polygon (~instant)
4. Annotation is automatically created and selected

If clicking without hovering, SAM decoder runs inference (~100-300ms) then creates the annotation.

## Quick Start

```tsx
import { SamTool, loadNpyEmbedding } from 'annota';

// Load precomputed embedding
const embedding = await loadNpyEmbedding('/embeddings/image_001.npy');

// Create SAM tool
const samTool = new SamTool({
  decoderModelUrl: '/models/sam_onnx_quantized_vit_b.onnx',
  embedding,
  imageWidth: 1024,
  imageHeight: 1024,
  annotationProperties: {
    classification: 'positive',
  }
});

// Initialize the model
await samTool.initializeModel();

// Activate the tool
annotator.setTool(samTool);
```

## Installation

SAM support is built-in to Annota. Download the required models:

### ONNX Decoder Models (for browser inference)

Download one of the quantized decoder models (~4.5MB each):

```bash
# ViT-B decoder (recommended for web - good balance of speed and quality)
wget https://bitrepo.oss-cn-shanghai.aliyuncs.com/models/sam/sam_onnx_quantized_vit_b.onnx

# ViT-H decoder (higher quality, same size due to quantization)
wget https://bitrepo.oss-cn-shanghai.aliyuncs.com/models/sam/sam_onnx_quantized_vit_h.onnx
```

Place in your `public/models/` directory.

### Python Encoder Models (for generating embeddings)

Download the corresponding PyTorch checkpoint for embedding generation:

```bash
# ViT-B encoder (~358MB - recommended)
wget https://bitrepo.oss-cn-shanghai.aliyuncs.com/models/sam/sam_vit_b_01ec64.pth

# ViT-H encoder (~2.4GB - highest quality)
wget https://bitrepo.oss-cn-shanghai.aliyuncs.com/models/sam/sam_vit_h_4b8939.pth
```

**Model Pairing:**
- Use `sam_vit_b_01ec64.pth` (Python) with `sam_onnx_quantized_vit_b.onnx` (browser)
- Use `sam_vit_h_4b8939.pth` (Python) with `sam_onnx_quantized_vit_h.onnx` (browser)

**Note:** The encoder models are only needed once to generate embeddings. After embeddings are created, you only need the ONNX decoder for browser inference.

**Alternative Sources:**
- Official Meta repository: [github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)
- All models are also mirrored on our OSS bucket for faster downloads in Asia-Pacific regions

## Generating Embeddings

**Important**: SAM inference runs entirely in TypeScript/browser, but you need Python to pre-generate embeddings (one-time per image).

```python
import torch
import numpy as np
from segment_anything import sam_model_registry, SamPredictor

# Load SAM model (one-time setup)
# Use "vit_b" for sam_vit_b_01ec64.pth or "vit_h" for sam_vit_h_4b8939.pth
sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b_01ec64.pth")
sam.to(device='cuda')  # or 'cpu' if no GPU
predictor = SamPredictor(sam)

# Load your image
import cv2
image = cv2.imread("image.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Generate embedding
predictor.set_image(image)
embedding = predictor.get_image_embedding().cpu().numpy()

# Save as .npy file
np.save("embeddings/image_001.npy", embedding)
```

**Tip:** Use the provided script for batch processing:

```bash
bash scripts/gen_sam_embeddings.sh -m vit_b \
  -i docs/public/playground/images/test \
  -o docs/public/playground/embeddings/test
```

After embeddings are generated, everything runs in TypeScript with no Python required.

## Hover Preview & Click-to-Create

The SAM tool features a real-time hover preview that dramatically improves user experience:

**Preview Behavior:**
- Hover over any object → blue overlay appears showing predicted segmentation
- Preview updates in real-time as you move the mouse (throttled to 50ms)
- Move mouse outside image bounds → preview disappears

**Click-to-Create:**
- Click on the preview → instantly creates polygon annotation
- The cached preview mask is reused (no redundant inference)
- Clicking within 5 pixels of the last preview position uses the cache
- Annotation matches the preview shape exactly

**Customization:**

```tsx
const samTool = new SamTool({
  decoderModelUrl: '/models/sam_onnx_quantized_vit_b.onnx',
  embedding,
  imageWidth: 1024,
  imageHeight: 1024,

  // Disable hover preview
  showHoverPreview: false,

  // Adjust preview transparency
  previewOpacity: 0.3,  // 0.0 = invisible, 1.0 = opaque
});
```

**Performance Notes:**
- Preview caching eliminates ~100-300ms inference delay on click
- Only the largest contour is converted to polygon (prevents fragment annotations)
- Preview runs at 20 FPS (50ms throttle) for smooth interaction

## API Reference

### SamTool

```tsx
interface SamToolOptions {
  // Model configuration
  decoderModelUrl: string;
  embedding: ort.Tensor;
  imageWidth: number;
  imageHeight: number;

  // Preview options
  showHoverPreview?: boolean;      // Show preview on hover (default: true)
  previewOpacity?: number;          // Preview overlay opacity (default: 0.5)

  // Annotation options
  annotationProperties?: Partial<Annotation>;

  // Callbacks
  onAnnotationCreated?: (annotation: Annotation) => void;
  onPredictionStart?: () => void;
  onPredictionComplete?: (iouScore: number) => void;
  onError?: (error: Error) => void;
}
```

**Methods:**
- `initializeModel()`: Initialize ONNX model (must be called before use)
- `isModelInitialized()`: Check if model is ready
- `setEmbedding(embedding, width, height)`: Update embedding for new image
- `destroy()`: Clean up resources

### Embedding Utilities

```tsx
// Load embedding from .npy file
const embedding = await loadNpyEmbedding('/embeddings/image.npy');

// Load with caching
const embedding = await loadNpyEmbeddingCached('/embeddings/image.npy');

// Create dummy embedding for testing
const dummyEmbedding = createDummyEmbedding();

// Manage cache
import { embeddingCache } from 'annota';
embeddingCache.clear();
embeddingCache.getSize();
embeddingCache.delete('/embeddings/old.npy');
```

## FAQ

### Do I need Python?

**Short answer**: Only once, to generate embeddings. After that, everything is TypeScript.

### Why can't I generate embeddings in TypeScript?

The SAM encoder model is ~350MB (too large for browser) and computationally expensive (~5-10 seconds per image even on GPU). Phase 3 will add browser-based encoding.

### How much disk space do embeddings take?

Each embedding is ~4MB (`[1, 256, 64, 64]` float32 tensor).
- 100 images: ~400 MB
- 1000 images: ~4 GB

### Does the preview affect performance?

No. The preview runs inference on-demand while hovering, but the result is cached. When you click, the cached mask is instantly reused, making annotation creation feel instant (~0ms inference vs ~100-300ms without cache).

### Can I disable the hover preview?

Yes, set `showHoverPreview: false` in the SamTool options. The tool will still work - clicking will trigger inference and create annotations, just without the preview.

## Related

- [Annotation Tools](/docs/guides/tools) - Overview of all available tools
- [Meta SAM Repository](https://github.com/facebookresearch/segment-anything) - Official implementation
- [ONNX Runtime Web](https://onnxruntime.ai/docs/tutorials/web/) - Run ML models in browser
